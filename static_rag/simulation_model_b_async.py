#!/usr/bin/env python3
"""
Team 2: Static RAG Simulation (Async Version)
asyncio + aiohttpë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ì‹¤í–‰ìœ¼ë¡œ ì†ë„ í–¥ìƒ
"""

import os
import sys
import json
import pandas as pd
import random
import asyncio
from openai import AsyncOpenAI

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from utils.persona_generator import generate_balanced_personas, Persona
from utils.search_queries import GAMER_TYPE_QUERIES, GENERAL_QUERY
from utils.llm_config import get_llm_client, TEMPERATURE
from static_rag.rag_modules import RAGRetriever

# ë³‘ë ¬ í† í¬ë‚˜ì´ì € ê²½ê³  ì–µì œ
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ë³‘ë ¬ í† í¬ë‚˜ì´ì € ê²½ê³  ì–µì œ
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Async OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
def get_async_llm_client():
    """ë¹„ë™ê¸° LLM í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found in .env file.")
    
    client = AsyncOpenAI(api_key=api_key)
    model_name = "gpt-4o-mini"
    return client, model_name

client, MODEL_NAME = get_async_llm_client()
print(f"âœ… Using model: {MODEL_NAME} (Team 2 - Async)")

OUTPUT_FILE = "static_rag/Team2_StaticRAG_Results.csv"
SIMULATION_DATES_FILE = "datasets/simulation_dates.csv"

# =============================================================================
# 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
# =============================================================================

def create_prompt(agent: Persona, current_date: str, context: list):
    context_str = "\n".join(context) if context else "(No reviews found.)"
    
    return f"""[ROLE]
You are a {agent.age} {agent.gender}.
Personality: '{agent.gamer_type_name_display}' ({agent.description})

[DATE]
Today is {current_date}.

[SEARCH RESULTS]
Reviews selected based on your interests and recentness (Time-Weighted):
{context_str}

[TASK]
Decide to buy 'Cyberpunk 2077' or not based strictly on the reviews above.
- The reviews are filtered by relevance and recency.
- Trust these reviews as the most important information available to you.

[OUTPUT]
JSON only:
{{
    "decision": "YES" or "NO",
    "reasoning": "Explain why based on the reviews."
}}
"""

# =============================================================================
# 3. ë¹„ë™ê¸° API í˜¸ì¶œ
# =============================================================================

async def call_llm_async(client: AsyncOpenAI, prompt: str, semaphore: asyncio.Semaphore) -> dict:
    """ë¹„ë™ê¸° LLM í˜¸ì¶œ (ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ, ì¬ì‹œë„ í¬í•¨)"""
    async with semaphore:
        for attempt in range(3):
            try:
                res = await client.chat.completions.create(
                    model=MODEL_NAME, 
                    messages=[{"role": "system", "content": prompt}],
                    response_format={"type": "json_object"},
                    temperature=TEMPERATURE,
                    timeout=60  # ì´ˆê³¼ ì§€ì—° ë°©ì§€
                )
                return json.loads(res.choices[0].message.content)
            except Exception as e:
                if attempt == 2:
                    print(f"[LLM Final Error] {e}", flush=True)
                    return {"decision": "NO", "reasoning": f"Error: {e}"}
                await asyncio.sleep(2 ** attempt)

# =============================================================================
# 4. ë©”ì¸ ì‹¤í–‰ (ë¹„ë™ê¸°)
# =============================================================================

async def run_experiment_b_rag_async(n_per_type: int = 13, max_concurrent: int = 5):
    import sys
    sys.stdout.reconfigure(line_buffering=True)  # ì¦‰ì‹œ ì¶œë ¥
    sys.stderr.reconfigure(line_buffering=True)
    
    # ì§„í–‰/ì €ì¥ìš© ìƒíƒœ
    flush_every = 10  # ìš”ì²­ 10ê°œë§ˆë‹¤ CSV ì €ì¥
    buffer = []
    written = 0
    yes_count = 0
    no_count = 0
    flush_lock = asyncio.Lock()
    
    print("=" * 70, flush=True)
    print(f"Task 2: Static RAG Simulation (Async)", flush=True)
    print("=" * 70, flush=True)

    # RAG ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    print("Initializing RAG Retriever...")
    retriever = RAGRetriever()

    # ë‚ ì§œ ë¡œë“œ
    dates_df = pd.read_csv(SIMULATION_DATES_FILE)
    simulation_dates = dates_df['date'].tolist()
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    personas = generate_balanced_personas(n_per_type=n_per_type) 
    print(f"Generated {len(personas)} agents.")
    print(f"Total tasks: {len(simulation_dates)} dates Ã— {len(personas)} agents = {len(simulation_dates) * len(personas)} decisions")
    print(f"Max concurrent requests: {max_concurrent}\n")

    # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œì„ ìœ„í•œ Semaphore
    semaphore = asyncio.Semaphore(max_concurrent)
    retrieval_lock = asyncio.Lock()  # ChromaDBëŠ” thread-safe ì•„ë‹˜
    
    total_steps = len(simulation_dates) * len(personas)
    
    async def process_agent_date(persona: Persona, date_str: str, step_num: int):
        """ì—ì´ì „íŠ¸-ë‚ ì§œ ì¡°í•© ì²˜ë¦¬"""
        nonlocal completed
        if step_num <= 3:
            print(f"ğŸŸ¢ Start task {step_num}: {persona.id} @ {date_str}", flush=True)
        
        # 1. ì¿¼ë¦¬ ì„ ì •
        agent_queries = GAMER_TYPE_QUERIES.get(persona.gamer_type, [])
        selected_queries = []
        if len(agent_queries) >= 4:
            selected_queries = random.sample(agent_queries, 4)
        else:
            selected_queries = agent_queries
        selected_queries.append(GENERAL_QUERY)
        
        try:
            # 2~3. ChromaDB ê²€ìƒ‰ (ì§ë ¬í™”)
            async with retrieval_lock:
                candidates = []
                for query in selected_queries:
                    # Run blocking sync DB call in a separate thread to avoid blocking event loop
                    reviews = await asyncio.to_thread(retriever.retrieve_reviews, query, date_str, top_k=2)
                    candidates.extend(reviews)
                unique_candidates = list(set(candidates))
                final_docs = unique_candidates[:5]
                prompt = create_prompt(persona, date_str, final_docs)
            if step_num <= 3:
                print(f"ğŸ” Retrieved {len(prompt.splitlines())} lines of context for {persona.id} @ {date_str}", flush=True)
            
            # 4. ë¹„ë™ê¸° LLM í˜¸ì¶œ
            res = await asyncio.wait_for(call_llm_async(client, prompt, semaphore), timeout=120)
            
            decision = res.get("decision", "NO").upper()
            decision = "YES" if "YES" in decision else "NO"
            
            return {
                "Agent_ID": persona.id,
                "Name": persona.name,
                "Persona_Type": persona.gamer_type_name_display,
                "Decision": decision,
                "Simulation_Date": date_str,
                "Reasoning": res.get("reasoning", "")
            }
        except Exception as e:
            print(f"[Task Error] {persona.id} @ {date_str}: {e}", flush=True)
            return {
                "Agent_ID": persona.id,
                "Name": persona.name,
                "Persona_Type": persona.gamer_type_name_display,
                "Decision": "NO",
                "Simulation_Date": date_str,
                "Reasoning": f"Error: {e}"
            }
    
    # ëª¨ë“  ì‘ì—… ìƒì„±
    tasks = []
    step_count = 0
    for date_str in simulation_dates:
        for persona in personas:
            step_count += 1
            tasks.append(process_agent_date(persona, date_str, step_count))
    
    print(f"ğŸš€ Starting {len(tasks)} async tasks...\n")
    
    async def flush_buffer():
        nonlocal buffer, written
        if not buffer:
            return
        batch = buffer
        buffer = []
        def write_batch():
            header = not os.path.exists(OUTPUT_FILE) or os.path.getsize(OUTPUT_FILE) == 0
            pd.DataFrame(batch).to_csv(
                OUTPUT_FILE,
                mode="a",
                index=False,
                header=header,
                encoding="utf-8-sig"
            )
        async with flush_lock:
            await asyncio.to_thread(write_batch)
            written += len(batch)
            print(f"ğŸ’¾ Saved {written}/{total_steps} rows", flush=True)
    
    completed = 0
    for coro in asyncio.as_completed(tasks):
        result = await coro
        completed += 1
        if result["Decision"] == "YES":
            yes_count += 1
        else:
            no_count += 1
        buffer.append(result)
        
        total_done = written + len(buffer)
        if len(buffer) >= flush_every:
            await flush_buffer()
        if total_done % max(10, flush_every) == 0 or total_done == total_steps:
            progress_pct = total_done * 100 // total_steps
            print(f"âœ… Completed: {total_done}/{total_steps} ({progress_pct}%)", flush=True)
    
    # ë‚¨ì€ ë²„í¼ í”ŒëŸ¬ì‹œ
    await flush_buffer()
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    total = yes_count + no_count
    decision_counts = {
        "YES": yes_count / total if total else 0,
        "NO": no_count / total if total else 0
    }
    
    print("\n" + "=" * 70)
    print("Decision")
    print(f"NO     {decision_counts.get('NO', 0):.3f}")
    print(f"YES    {decision_counts.get('YES', 0):.3f}")
    print("=" * 70)
    print(f"Simulation completed. Results saved to {OUTPUT_FILE}")
    print("=" * 70)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    asyncio.run(run_experiment_b_rag_async(n_per_type=13, max_concurrent=20))

if __name__ == "__main__":
    main()

